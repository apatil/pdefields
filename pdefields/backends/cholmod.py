# -*- coding: UTF-8 -*-
"""
A linear algebra backend that uses sparse, cholesky-based operations.
"""
__all__ = ['into_matrix_type', 'precision_to_products', 'pattern_to_products', 'rmvn', 'mvn_logp', 'axpy', 'dm_solve_m', 'm_mul_m', 'm_xtyx', 'eta', 'conditional_mean_and_precision']

import numpy as np
import scipy
from scipy import sparse
import scikits.sparse.cholmod as cholmod

precision_product_keys = ['L','P','F']

def into_matrix_type(m):
    "Takes a matrix m and returns a representation of it as a SciPy compressed sparse column matrix. This is the matrix format used by the CHOLMOD wrapper in scikits.sparse."
    return sparse.csc.csc_matrix(m)
    
def axpy(a,x,y):
    "a,x,y -> ax+y"
    return a*x + y

def dm_solve_m(x,y):
    "A^{-1} B, where A is diagonal and B is CSC."
    x_i = x.copy()
    x_i.data = 1./x_i.data
    return x_i * y

def m_xtyx(x,y):
    "x,y -> x^T y x ."
    # Do it this way to stay in CSC format.
    return x.__rmul__(x.T*y)

def m_mul_m(x,y):
    "x,y -> xy"
    return x*y
    
def pattern_to_products(pattern):
    """Takes a sparse matrix with the correct sparsity pattern, but not necessarily meaningful values, and returns the symbolic Cholesky factorization of it, computed by CHOLMOD via scikits.sparse. The symbolic factorization is stored in a Factor object. Its method P can be called to obtain the permutation vector. I don't know if there's any way to get the actual sparsity pattern out, but it can surely be done.
    
    The return value is stored in a singleton dictionary with one key, 'symbolic'. It is stored in a dictionary to make it possible to have a uniform return type across all backends."""
    return {'symbolic': cholmod.analyze(pattern)}

def precision_to_products(Q, diag_pert, symbolic):
    """
    Takes a sparse precision matrix Q and a symbolic Cholesky factorization 'symbolic' and returns several derivative products in a dictionary:
    - Q: The input precision matrix, unaltered.
    - F: The Factor object generated by scikits.sparse. This object supports quick and easy triangular solves.
    - det: The determinant of Q.
    - P: A permutation vector. Cholmod computes the Cholesky factorization LDL^T = Q[P,:][:,P].
    - Pbak: The backward permutation vector. x[P][Pbax] = x, and (LDL^T)[Pbak,:][:,Pbak] = Q
    - sqrtD: sqrt(D).
    - """
    # FIXME: This should be symbolic.cholesky(Q)... but that doesn't work when alpha=2. Why?
    # F = cholmod.cholesky(Q)
    F = symbolic.cholesky(Q,beta=diag_pert)
    D = F.D()
    sqrtD = np.sqrt(D)
    det = np.sum(np.log(D))
    P = symbolic.P()
    Pbak = np.argsort(P)
    return {'det':det, 'P': P, 'F': F, 'Pbak': Pbak, 'Q': Q, 'sqrtD': sqrtD}

def rmvn(M,Q,det,F,P,Pbak,sqrtD):
    """
    Takes the following:
    - M: A mean vector
    - Q: A sparse precision matrix
    - det: The determinant of Q
    - F: A scikits.sparse Factor object representing the Cholesky factorization of Q
    - P: A permutation vector
    - Pbak: The permutation vector that inverts P.
    - sqrtD: The square root of the diagonal matrix D from Cholmod's Cholesky factorization.
    Returns a draw from the multivariate normal distribution ith mean M and precision P
    """
    return M + F.solve_Lt(np.random.normal(size=len(M)) / sqrtD)[Pbak]

def mvn_logp(x,M,Q,det,F,P,Pbak,sqrtD):
    """
    Takes the following:
    - x: A candidate value as a vector.
    - M: A mean vector
    - Q: A sparse precision matrix
    - det: The determinant of Q
    - F: A scikits.sparse Factor object representing the Cholesky factorization of Q
    - P: A permutation vector
    - Pbak: The permutation vector that inverts P.
    - sqrtD: The square root of the diagonal matrix D from Cholmod's Cholesky factorization.    
    Returns the log-probability of x given M and Q.
     """
    d = (x-M)[P]
    return -.5*np.dot(d,Q*d) + .5*det - .5*len(M)*np.log(2.*np.pi)

def eta(M,Q,det,F,P,Pbak,sqrtD):
    u"""
    Takes the following:
    - x: A candidate value as a vector.
    - M: A mean vector
    - Q: A sparse precision matrix
    - det: The determinant of Q
    - F: A scikits.sparse Factor object representing the Cholesky factorization of Q
    - P: A permutation vector
    - Pbak: The permutation vector that inverts P.
    - sqrtD: The square root of the diagonal matrix D from Cholmod's Cholesky factorization.    
    Returns the "canonical mean" Î·=Q M.
     """
    return Q*M        

def id(x):
    return x    
    
def conditional_mean_and_precision(y,M,Q,Q_obs,L_obs=None,K_obs=None,symbolic=None):
    """    
    Returns the conditional mean and precision of x in the conjugate submodel
    
    x ~ N(M,Q^{-1})
    y ~ N(L_obs x + K_obs, Q_obs^{-1})
    
    Takes:
    - Observed value of y
    - Mean vector M
    - Sparse SPD matrices Q and Qobs
    - Sparse matrix, dense matrix or LinearOperator L_obs (optional)
    - Offset vector K_obs (optional)
    - Symbolic Cholesky factorization previously returned by conditional_mean_and_precision (optional)
    
    Returns:
    - Conditional precision as a CSC matrix
    - Symbolic Cholesky factorization of conditional precision as a Cholmod Factor
    - Numeric "
    - Conditional mean 
    """
    # Note that the joint precision is
    # [Q + L_obs' Q_obs L_obs      -L_obs' Q_obs]
    # [-Q_obs L_obs                 Q_obs]
    
    # L_obs defaults to the identity operator
    if L_obs is None:
        from scipy.sparse import linalg
        L_obs = linalg.LinearOperator((len(y),len(x)), id, id)
        
    # K_obs defaults to the zero vector.
    if K_obs is None:
        K_obs = 0*y
    
    # Conditional precision.
    Qc = Q+(Q_obs*L_obs).__rmul__(L_obs.T)
    
    # If no symbolic factorization was provided, compute it here.
    if symbolic is None:
        symbolic = cholmod.analyze(Qc)
    
    # Numeric factorization and conditional mean.
    delta = y-L_obs*M-K_obs
    numeric = symbolic.cholesky(Qc)
    Mc = M + numeric.solve_A(L_obs.T*Q_obs*delta).reshape(M.shape)

    return Qc, symbolic, numeric, Mc
    
# if __name__ == '__main__':
#     n = 100
#     nobs = 50
#     B = np.random.normal(size=(n,n)).view(np.matrix)
#     Q = B*B.T
#     L = np.random.normal(size=(n,nobs)).view(np.matrix)
#     M = np.random.normal(size=n).reshape((-1,1))
#     
#     B = np.random.normal(size=(nobs,nobs)).view(np.matrix)
#     Qobs = B*B.T
#     
#     Cx = Q.I
#     Cxy = L.T * Cx
#     Cy = L.T* Cx * L + Qobs.I
#     Ccombo = np.bmat([[Cx,Cxy.T],[Cxy,Cy]])
# 
#     Qcombo = Ccombo.I
#     Qx = Q + L*Qobs*L.T
#     Qxy = -L*Qobs
#     # Qcombo_ = np.bmat([[Qx,Qxy],[Qxy.T,Qobs]])
#     
#     # Equivalent.
#     Ccond = Cx-Cxy.T*Cy.I*Cxy
#     # Ccond_try = Qcombo_[:n,:n].I
#     
#     xobs = L.T*M + np.dot(np.linalg.cholesky(Cy), np.random.normal(size=nobs)).reshape((-1,1))
#     Mcond = M+Cxy.T*Cy.I*(xobs-L.T*M)
#     
#     # eta = Qcombo_*np.vstack((M,L.T*M))
#     # etacond = eta[:n]-Qxy*xobs
#     # Mcond_try = Qx.I*etacond
#     # Mcond_try = M+Qx.I*L*Qobs*(xobs-L.T*M)
#     
#     Qx_try, symbolic, numeric, Mcond_try = conditional_mean_and_precision(xobs, M, *map(sparse.csc_matrix, [Q, Qobs, L.T]))
#     
#     print np.abs(Mcond_try.view(np.ndarray).ravel()-Mcond.view(np.ndarray).ravel()).max()
#     